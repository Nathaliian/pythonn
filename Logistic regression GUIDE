import os
os.getcwd()
'C:\\Users\\USER'
#changing working directory 
os.chdir("C:\\Users\\USER\\OneDrive\\Desktop\\MSC\\Python data") 
os.getcwd()
'C:\\Users\\USER\\OneDrive\\Desktop\\MSC\\Python data'
import pandas as pd
from matplotlib import pyplot as plt
import seaborn as sns
import statsmodels.api as sm

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
data1= pd.read_csv("car_data.csv",index_col="User ID")
data1.head()

data1.hist()
array([[<Axes: title={'center': 'Age'}>,
        <Axes: title={'center': 'AnnualSalary'}>],
       [<Axes: title={'center': 'Purchased'}>, <Axes: >]], dtype=object)

#check missing values - #data.dropna()-to drop
data1.isnull().any()
Gender          False
Age             False
AnnualSalary    False
Purchased       False
dtype: bool
#Convert gender into a dummy variable
data1['Gender_d']=data1['Gender'].replace({'Male':0,'Female':1})
data1=data1.drop("Gender", axis=1)
data1.head(1)
Age	AnnualSalary	Purchased	Gender_d
User ID				
385	35	20000	0	0
sns.pairplot(data1)
<seaborn.axisgrid.PairGrid at 0x1f9ffbde560>

y=data1['Purchased']
X=data1[['Gender_d','Age',
       'AnnualSalary']]
X=sm.add_constant(X)
log_reg=sm.Logit(y,X,data=data1).fit()
print(log_reg.summary())
Optimization terminated successfully.
         Current function value: 0.371479
         Iterations 7
                           Logit Regression Results                           
==============================================================================
Dep. Variable:              Purchased   No. Observations:                 1000
Model:                          Logit   Df Residuals:                      996
Method:                           MLE   Df Model:                            3
Date:                Fri, 19 May 2023   Pseudo R-squ.:                  0.4487
Time:                        15:31:03   Log-Likelihood:                -371.48
converged:                       True   LL-Null:                       -673.81
Covariance Type:            nonrobust   LLR p-value:                9.794e-131
================================================================================
                   coef    std err          z      P>|z|      [0.025      0.975]
--------------------------------------------------------------------------------
const          -11.8603      0.774    -15.324      0.000     -13.377     -10.343
Gender_d        -0.3184      0.186     -1.716      0.086      -0.682       0.045
Age              0.2195      0.015     14.471      0.000       0.190       0.249
AnnualSalary   3.37e-05   3.23e-06     10.426      0.000    2.74e-05       4e-05
================================================================================
C:\Users\USER\anaconda\lib\site-packages\statsmodels\base\model.py:127: ValueWarning: unknown kwargs ['data']
  warnings.warn(msg, ValueWarning)
C:\Users\USER\anaconda\lib\site-packages\statsmodels\base\model.py:127: ValueWarning: unknown kwargs ['data']
  warnings.warn(msg, ValueWarning)
import numpy as np
odds_ratio=pd.DataFrame(
    {
        "OR" : log_reg.params,
        "Lower CI" : log_reg.conf_int()[0],
        "Upper CI" : log_reg.conf_int()[1],
             
    }

)
odds_ratio=np.exp(odds_ratio)
print(odds_ratio)
                    OR  Lower CI  Upper CI
const         0.000007  0.000002  0.000032
Gender_d      0.727287  0.505587  1.046202
Age           1.245452  1.208972  1.283034
AnnualSalary  1.000034  1.000027  1.000040
# Using SCiKit learn
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report , confusion_matrix
df = pd.read_csv("car_data.csv",index_col = "User ID")
df.head()
Gender	Age	AnnualSalary	Purchased
User ID				
385	Male	35	20000	0
681	Male	40	43500	0
353	Male	49	74000	0
895	Male	40	107500	1
661	Male	25	79000	0
#Convert gender into a dummy variable
df['Gender_d']=df['Gender'].replace({'Male':0,'Female':1})
df=df.drop("Gender", axis=1)
df.head(1)
Age	AnnualSalary	Purchased	Gender_d
User ID				
385	35	20000	0	0
#use iloc function to define X and Y Variables
X = df.iloc[:,[0,1,3]]
Y = df.iloc[:,2]
X.head(1)
Age	AnnualSalary	Gender_d
User ID			
385	35	20000	0
# Feature scaling - to normalize the range of independence
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X = sc.fit_transform(X)
X
array([[-0.47711966, -1.52849808, -1.03252879],
       [-0.00990495, -0.84676745, -1.03252879],
       [ 0.83108153,  0.03803187, -1.03252879],
       ...,
       [-1.13122026,  1.90916486,  0.968496  ],
       [ 0.73763859,  1.77862069,  0.968496  ],
       [ 0.36386682,  0.02352696,  0.968496  ]])
# Splittinf data into test and train
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.3,random_state  = 42)
logistic_reg = LogisticRegression()
logistic_reg.fit(X_train, y_train)

LogisticRegression
LogisticRegression()
# predict the y variable in test category using the model derived above
y_pred = logistic_reg.predict(X_test)
#compare
result = pd.DataFrame({'Actual' : y_test, 'Predicted' : y_pred})
result
Actual	Predicted
User ID		
176	0	0
448	1	1
391	0	0
623	1	0
773	0	1
...	...	...
66	1	1
539	0	0
71	0	0
588	0	0
83	1	1
300 rows Ã— 2 columns

logistic_reg
#Confusion matrix
from sklearn.metrics import confusion_matrix , accuracy_score , precision_score, recall_score,f1_score
#import the metrics class
cnf_matrix=confusion_matrix(y_test,y_pred)
cnf_matrix
array([[157,  15],
       [ 42,  86]], dtype=int64)
sns.heatmap(pd.DataFrame(cnf_matrix),cmap="coolwarm",annot=True)
plt.title('Confusion Matrix' ,y=1.1)
plt.ylabel('Actual Label')
plt.xlabel('Predicted Label')
Text(0.5, 23.52222222222222, 'Predicted Label')

#ACCURACY

# TN+TP/(TP+TN+FP+FN)

accuracy = accuracy_score(y_test,y_pred)
accuracy
0.81
# PRECISION
# Out of all the total positive ,what % is truly positive
# precision = TP/(TP+FP)

precision = precision_score(y_test,y_pred)
precision
0.8514851485148515
# RECALL
# Out of the total positive,what % are predicted positive values
# RECALL = TP/(TP+FN)

recall = recall_score(y_test,y_pred)
recall
0.671875
# F1 score - balanced F-score or F- measure
# A harmonic mean of the precision and recall
# Best value 1, worst 0
#F1 =2*(PRECISION*RECALL) / (PRECISION + RECALL)

f1 = f1_score(y_test,y_pred)
f1
0.7510917030567686
